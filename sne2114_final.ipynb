{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sne2114_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SalNel97/qmss_python_final/blob/main/sne2114_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jIXJNuMHFmH"
      },
      "source": [
        "**QMSS S5073**\n",
        "# ***Final***\n",
        "**Salah El-Sadek (sne2114)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrJv8I9Gysbw"
      },
      "source": [
        "#**Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6EZb7rgy1L4"
      },
      "source": [
        "*From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0tRT_eqMwI"
      },
      "source": [
        "**In general, regression models in linear regression (for continuous outcome variables) and logistic regression (for binary outcome variables) are the ones most useful for social scientists in terms of being able to interpret coefficients and, by extension, be able to have realistic interpretations of the relationships between variables in our model. Unpenalized linear or logistic regression models can be run, or an L1 (Lasso) or L2 (Ridge) penalty can be applied to minimize RSS (and variance) for better prediction or for the ability to eliminate irrelevant variables from the model (only in the case of Lasso penalty are you able to completely eliminate predictors from final model). It is the ability to be able to interpret coefficients that allow for any meaningful statisitical hypothesis testing to be performed, which is generally the main focus of a social scientist.**\n",
        "\n",
        "**One could also explore the data using decision tree type models (Random Forest, Boosted, Bagged) to identify any interaction associations between variables and which variables are of importance in predicting certain aspects of the data set.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcCoLWvFys26"
      },
      "source": [
        "#**Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqP120KGy2j8"
      },
      "source": [
        "*Describe the main differences between supervised and unsupervised learning.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyEgDeTpdgJ2"
      },
      "source": [
        "**Supervised learning involves models trained and tested to be good at predicting an outcome Y from a different set of X predictors. While unsupervised learning involves only dealing with our X predictors without concern for prediction ability of certain outcome variable Y.**\n",
        "\n",
        "**Due to this difference, unsupervised learning is much more open-ended without a set goal/prediction score in mind. Just with the aim of discovering interesting charactersitics defining our set of X predictor variables using only the X variables in said process. Supervised learning, on the other hand, is much more directed as it is goverened by the goal that the model is able to predict our Y outcome well and which X predictors are involved in such predictions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfLyqKOnys-6"
      },
      "source": [
        "#**Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfCNm0vzy6Uq"
      },
      "source": [
        "*Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lXrcEO0ixHt"
      },
      "source": [
        "**Supervised learning is the primary approach used by machine learning practitioners as we are concerned with the predictive power of a certain response outcome variable.**\n",
        "\n",
        "**Secondary to that approach are unsupervised learning methods which only use the X predictor variables to discover interesting relationships among said X variables. This could involve analysing the presence of any meaningful clusters or homogenous groupings for our X predictors, or exploring the most important dimensions/features to focus on that explain non-random variation in the data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgwNW_cUytFh"
      },
      "source": [
        "#**Question 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SclzlssPy-ap"
      },
      "source": [
        "*Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fInb4Rz8kSp9"
      },
      "source": [
        "**Some of these approaches include Principle Component Analysis (PCA), Manifold Learning (MDS, LLE, or IsoMap), and clustering techniques such as K-Means Clustering and Hierarchical/Agglomerative Clustering.**\n",
        "\n",
        "**Clustering methods involve finding homogenous subgroups within our data by either partitioning our data into a predefined number of 'K' clusters, or by performing step-wise leaf-to-branch analysis and gradually grouping data points in groups and subgroups until all points are in a group/cluster. The later method is more flexible in that it does not require a predefined number of 'K' cluster (a parameter which would have to be tuned). In the end, these kinds of clustering techniques are concerned with finding relationships between X predictors in terms of their homogeneity when partitioned/labeled into certain subgroups, and the similarities/differences between said subgroups.**\n",
        "\n",
        "**PCA is concerned with reducing the number of dimensions of our data to the minimal required to explain the majority of the variance in our data points. Given its concern with dimension reductionality only, in comparision to clustering techniques like K-means, PCA does not particularly require apriori knowledge/background to determine parameters unlike choosing the number of 'K' cluster to use.**\n",
        "\n",
        "**Manifold learning is a useful technique besides PCA in that it performs well in detecting nonlinear relationships in the data. But some disadvantages that leave PCA as being more viable include Manifold learning techniques' inability to handle missing data, the need for a pre-set number of neighbors for distant matrix calculations, and even the inability to filter out noise automatically (like PCA).** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrPwaSoJytLn"
      },
      "source": [
        "#**Question 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flxQKEaizC5c"
      },
      "source": [
        "*What are the main benefits of using Principal Components Analysis?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLlKieOqsiy"
      },
      "source": [
        "**Given PCA fundemental ability to reduce data to a fewer number dimensions while still explaining a good portion of the variability in your data all at once there are multiple uses for it:**\n",
        "* **Focusing on most important feature variables in data set for future supervised learning/hypothesis testing.**\n",
        "\n",
        "* **Filtering noise out of your data as a preprocessing method by flipping the PCA method and looking at the largest subset of components (signal) to help us isolate the smaller component subset that is the noise.**\n",
        "\n",
        "* **The ability to visualize data with a very high number of dimensions such as image data, and the ability to effectively filter noise out of these higher dimensional image data.**\n",
        "\n",
        "**In general, PCA is very effective at focusing on the most important features/dimensions (for various reasons), but especially when we talk about high-dimensional data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAN3SyHRytR0"
      },
      "source": [
        "#**Question 6**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybedca1VzJmZ"
      },
      "source": [
        "*Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlA2bKfiieJ8"
      },
      "source": [
        "* **One of the major differences between DMPN and CNN is that DMPN is a feedfoward neural network only while CNN is not. This means that the nodes connecting our input layers to our hidden layers to our output layer do not form a loop. The information simply goes in one direction from input layers through hidden layers and onto prediction with the final output layer. Looping between layers is possible with the convolutional layers in CNN.**\n",
        "\n",
        "* **Another difference is the fact that DMPN uses 'dense' fully-connected layers in which each node is connected to every other node in a nearby layer. While in CNN we have sparsely connected layers, where every node is not necessarily connected to every other node. This has the benefits of greatly reducing the total number of parameters in our network and reducing the redunduncies inherent in high-dimensional data. This makes CNN more efficient and able to predict output layers (images, probabilty distributions, etc.) while requiring a smaller number of input variables compared to DMPN.**\n",
        "\n",
        "* **CNN has the ability of reading inputs as matrices as well as vectors while DMPN can only take vector-type inputs. This has the consequence of DMPN not having specific spatial-dependence when identifying patterns (if talking about pixels in an image, for example) since relationships are determined by the pixel of an image (given the dense fully-connected nature of DMPN layers). But with the ability to read in matrix inputs, certain image patterns and feature detectors are 'shared' in other spatial positions of an image. CNN is a \"smarter\" image prediction method as it does not try to predict output nodes by meticulously utilizing all input information available, but instead realizes that certain shape/edge prediction patterns are common in different areas of the image and utilizes that fact in its image prediction. CNN is more efficient and useful for very high-dimension complex image or audio data, for instance.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eve6b5GUjFMZ"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "# Importing relevant libraries\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtzt2IX2uWNa"
      },
      "source": [
        "#**Question 7**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npNfe8V2yHcs"
      },
      "source": [
        "*Write the keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model.  You will not run it on real data.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-fr8xD6TloY",
        "outputId": "b06f8f81-514e-4b94-b7bd-cb9226fb079c"
      },
      "source": [
        "model1 = Sequential() \n",
        "model1.add(Dense(units = 50, activation = 'relu', input_dim = 7))\n",
        "model1.add(Dense(units = 100, activation = 'relu'))\n",
        "model1.add(Dense(units = 150, activation = 'relu'))\n",
        "model1.add(Dense(units = 5, activation = 'softmax'))\n",
        "\n",
        "# Compile\n",
        "model1.compile(loss='binary_crossentropy', optimizer = 'sgd', metrics=['accuracy']) #Could use AUC instead of accuracy\n",
        "  \n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                400       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               5100      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 150)               15150     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 755       \n",
            "=================================================================\n",
            "Total params: 21,405\n",
            "Trainable params: 21,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi_5TyTDufea"
      },
      "source": [
        "#**Question 8**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0Dr6oXyLN5"
      },
      "source": [
        "*Write the keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model.  You will not run it on real data.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RAeCuCEufea",
        "outputId": "226ef40c-e4b1-4a0a-eb9a-91c9d87fba5f"
      },
      "source": [
        "model2 = Sequential() \n",
        "model2.add(Dense(units = 75, activation = 'relu', input_dim = 7))\n",
        "model2.add(Dense(units = 150, activation = 'relu'))\n",
        "model2.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compile\n",
        "model2.compile(loss='binary_crossentropy', optimizer = 'sgd', metrics=['accuracy']) #Could use AUC instead of accuracy\n",
        "  \n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 75)                600       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 150)               11400     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 151       \n",
            "=================================================================\n",
            "Total params: 12,151\n",
            "Trainable params: 12,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkRx3Tn1zPfH"
      },
      "source": [
        "#**Question 9**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdsHJ35izSLM"
      },
      "source": [
        "*Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model.  You will not run it on real data.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd9k3cA3zcnI",
        "outputId": "dea36618-6d10-41b0-a7c1-f9df58accebf"
      },
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=[140, 140, 3]))\n",
        "model3.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model3.add(Conv2D(filters=28, kernel_size=2, padding='same', activation='relu'))\n",
        "model3.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile \n",
        "model3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 140, 140, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 70, 70, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 70, 70, 28)        1820      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 35, 35, 28)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 34300)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                343010    \n",
            "=================================================================\n",
            "Total params: 345,038\n",
            "Trainable params: 345,038\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLbo6us5zPrp"
      },
      "source": [
        "#**Question 10**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpB8HPWEzYD8"
      },
      "source": [
        "*Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model.  You will not run it on real data.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrsrWsx2zdEE",
        "outputId": "ccfe7ac6-cf15-45d5-ff4f-51acfd9f3a30"
      },
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', input_shape=[140, 140, 3]))\n",
        "model4.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model4.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model4.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(128, activation='relu'))\n",
        "model4.add(Dense(128, activation='relu')) \n",
        "\n",
        "model4.add(Dense(6, activation='softmax')) \n",
        "\n",
        "# Compile \n",
        "model4.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "model4.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 140, 140, 32)      416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 70, 70, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 70, 70, 32)        4128      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 35, 35, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 39200)             0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               5017728   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 5,039,558\n",
            "Trainable params: 5,039,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}